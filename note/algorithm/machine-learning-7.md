# 7.1 过拟合问题

## 什么是过拟合？

- 过拟合（overfitting）是指一个模型在训练集上表现很好，但是在测试集或新数据上表现很差的现象。
- 过拟合通常发生在模型过于复杂，拟合了训练集中的噪声或异常值，而没有捕捉到数据的真实规律。
- 过拟合的模型会导致高方差（high variance），即模型对不同数据集的预测结果有很大的差异。

## 如何判断过拟合？

- 一种判断过拟合的方法是使用交叉验证（cross validation）。
- 交叉验证是指将数据集分成k个子集，每次用其中一个子集作为测试集，其余的作为训练集，重复k次，得到k个测试误差和训练误差的平均值。
- 如果训练误差远小于测试误差，说明模型过拟合了训练集。
- 如果训练误差和测试误差都很大，说明模型欠拟合了数据，即模型过于简单，不能捕捉到数据的真实规律。
- 如果训练误差和测试误差都很小，说明模型拟合得很好。
- 如果训练误差略大于测试误差，说明模型可能有一些偏差（bias），即模型对数据有一些错误的假设或先验知识。

## 如何解决过拟合？

- 一种解决过拟合的方法是使用正则化（regularization）。
- 正则化是指在损失函数中加入一个惩罚项，使得模型的参数不能过大或过小，从而限制模型的复杂度。
- 常用的正则化方法有L1正则化和L2正则化。
- L1正则化是指在损失函数中加入参数的绝对值之和作为惩罚项，可以使得部分参数变为0，从而实现特征选择（feature selection）。
- L2正则化是指在损失函数中加入参数的平方之和作为惩罚项，可以使得参数变小，从而防止过度拟合（overfitting）。

# 7.2 代价函数

## 什么是代价函数？

- 代价函数（cost function）是指衡量模型预测结果和真实结果之间的差异的函数。
- 代价函数的值越小，说明模型的预测能力越好。
- 代价函数的选择取决于模型的类型和目标。

## 常用的代价函数有哪些？

- 常用的代价函数有均方误差（mean squared error，MSE）、交叉熵（cross entropy）、对数似然（log likelihood）等。
- 均方误差是指预测值和真实值之差的平方的平均值，适用于回归问题（regression problem）。
- 交叉熵是指真实值和预测值之间的信息熵（information entropy），适用于分类问题（classification problem）。
- 对数似然是指真实值和预测值之间的对数概率（log probability），适用于概率模型（probabilistic model）。

## 如何使用正则化修改代价函数？

- 使用正则化修改代价函数的方法是在原始的代价函数上加上一个正则化项（regularization term），即
$$
J(\theta) = J_{\text{original}}(\theta) + \lambda R(\theta)
$$
其中，$\theta$表示模型的参数，$\lambda$表示正则化系数，$R(\theta)$表示正则化函数。
- 正则化系数$\lambda$可以控制正则化项对代价函数的影响程度，$\lambda$越大，正则化项越重要，模型越简单；$\lambda$越小，正则化项越不重要，模型越复杂。
- 正则化函数$R(\theta)$可以选择不同的形式，例如L1正则化和L2正则化。
- L1正则化的正则化函数是参数的绝对值之和，即
$$
R(\theta) = \sum_{i} |\theta_i|
$$
- L2正则化的正则化函数是参数的平方之和，即
$$
R(\theta) = \sum_{i} \theta_i^2
$$


# 7.3 线性回归的正则化

## 什么是线性回归？

- 线性回归（linear regression）是一种用于预测连续值的模型，其假设输出变量和输入变量之间存在线性关系，即
$$
y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
$$
其中，$y$表示输出变量，$x_i$表示输入变量，$\theta_i$表示参数。
- 线性回归的目标是找到一组参数$\theta$，使得模型的预测值和真实值之间的均方误差最小，即
$$
\min_{\theta} J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2
$$
其中，$m$表示数据集的大小，$h_{\theta}(x)$表示模型的预测函数。

## 为什么要对线性回归进行正则化？

- 对线性回归进行正则化的目的是防止过拟合，即模型在训练集上表现很好，但是在测试集或新数据上表现很差的现象。
- 过拟合通常发生在模型过于复杂，拟合了训练集中的噪声或异常值，而没有捕捉到数据的真实规律。
- 对线性回归进行正则化的方法是在原始的代价函数上加上一个正则化项（regularization term），使得模型的参数不能过大或过小，从而限制模型的复杂度。

## 如何对线性回归进行正则化？

- 对线性回归进行正则化的方法是在原始的代价函数上加上一个正则化项（regularization term），即
$$
J(\theta) = J_{\text{original}}(\theta) + \lambda R(\theta)
$$
其中，$\lambda$表示正则化系数，$R(\theta)$表示正则化函数。
- 正则化系数$\lambda$可以控制正则化项对代价函数的影响程度，$\lambda$越大，正则化项越重要，模型越简单；$\lambda$越小，正则化项越不重要，模型越复杂。
- 正则化函数$R(\theta)$可以选择不同的形式，例如L1正则化和L2正则化。
- L1正则化的正则化函数是参数的绝对值之和，即
$$
R(\theta) = \sum_{i} |\theta_i|
$$
- L2正则化的正则化函数是参数的平方之和，即
$$
R(\theta) = \sum_{i} \theta_i^2
$$
- 对于线性回归来说，通常使用L2正则化（也称为岭回归（ridge regression）），因为它可以保证代价函数是凸函数（convex function），从而保证有唯一的最优解。
- 使用L2正则化后，线性回归的代价函数变为
$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \sum_{i=1}^n \theta_i^2
$$
其中，$n$表示输入变量的个数。
- 使用L2正则化后，线性回归的参数更新公式变为
$$
\theta_j := \theta_j - \alpha \left( \frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)} + \frac{\lambda}{m} \theta_j \right)
$$
其中，$\alpha$表示学习率（learning rate），$x_j^{(i)}$表示第$i$个样本的第$j$个输入变量。
- 注意，当$j=0$时，即$\theta_0$为截距项（intercept term）时，不需要进行正则化，因为它不影响模型的复杂度。
- 因此，当$j=0$时，参数更新公式为
$$
\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})
$$

# 7.4 逻辑回归的正则化

## 什么是逻辑回归？

- 逻辑回归（logistic regression）是一种用于预测二分类问题（binary classification problem）的模型，其假设输出变量和输入变量之间存在对数几率（log odds）的线性关系，即
$$
\log \frac{p}{1-p} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
$$
其中，$p$表示输出变量为正类（positive class）的概率，$x_i$表示输入变量，$\theta_i$表示参数。
- 逻辑回归的目标是找到一组参数$\theta$，使得模型的预测概率和真实概率之间的对数似然（log likelihood）最大，即
$$
\max_{\theta} L(\theta) = \sum_{i=1}^m y^{(i)} \log h_{\theta}(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_{\theta}(x^{(i)}))
$$
其中，$m$表示数据集的大小，$y^{(i)}$表示第$i$个样本的真实标签（0或1），$h_{\theta}(x)$表示模型的预测函数，为
$$
h_{\theta}(x) = \frac{1}{1 + e^{-\theta^T x}}
$$

## 为什么要对逻辑回归进行正则化？

- 对逻辑回归进行正则化的目的也是防止过拟合，即模型在训练集上表现很好，但是在测试集或新数据上表现很差的现象。
- 过拟合通常发生在模型过于复杂，拟合了训练集中的噪声或异常值，而没有捕捉到数据的真实规律。
- 对逻辑回归进行正则化的方法也是在原始的对数似然函数上加上一个正则化项（regularization term），使得模型的参数不能过大或过小，从而限制模型的复杂度。

## 如何对逻辑回归进行正则化？

- 对逻辑回归进行正则化的方法也是在原始的对数似然函数上加上一个正则化项（regularization term），即
$$
L(\theta) = L_{\text{original}}(\theta) - \lambda R(\theta)
$$
其中，$\lambda$表示正则化系数，$R(\theta)$表示正则化函数。
- 正则化系数$\lambda$可以控制正则化项对对数似然函数的影响程度，$\lambda$越大，正则化项越重要，模型越简单；$\lambda$越小，正则化项越不重要，模型越复杂。
- 正则化函数$R(\theta)$可以选择不同的形式，例如L1正则化和L2正则化。
- L1正则化的正则化函数是参数的绝对值之和，即
$$
R(\theta) = \sum_{i} |\theta_i|
$$
- L2正则化的正则化函数是参数的平方之和，即
$$
R(\theta) = \sum_{i} \theta_i^2
$$
- 对于逻辑回归进行正则化后，参数$\theta$的更新公式变为
$$
\theta_j := \theta_j - \alpha \left( \frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)} - \frac{\lambda}{m} g(\theta_j) \right)
$$
其中，$\alpha$表示学习率（learning rate），$x_j^{(i)}$表示第$i$个样本的第$j$个输入变量，$g(\theta_j)$表示正则化函数对参数$\theta_j$的导数（derivative）。
- 如果使用L1正则化，那么$g(\theta_j)$为
$$
g(\theta_j) = \text{sign}(\theta_j) = \begin{cases}
1 & \text{if } \theta_j > 0 \\
0 & \text{if } \theta_j = 0 \\
-1 & \text{if } \theta_j < 0
\end{cases}
$$
- 如果使用L2正则化，那么$g(\theta_j)$为
$$
g(\theta_j) = 2\theta_j
$$
- 注意，当$j=0$时，即$\theta_0$为截距项（intercept term）时，不需要进行正则化，因为它不影响模型的复杂度。
- 因此，当$j=0$时，参数更新公式为
$$
\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})
$$

## 正则化对逻辑回归的影响

- 正则化对逻辑回归的影响主要体现在模型的参数和决策边界（decision boundary）上。
- 模型的参数：正则化可以使模型的参数变得更小或更稀疏（sparse），从而降低模型的复杂度和方差（variance），提高模型的泛化能力（generalization ability）。
- 决策边界：正则化可以使模型的决策边界变得更平滑或更简单，从而避免过拟合训练集中的噪声或异常值，提高模型在测试集或新数据上的表现。
